<!DOCTYPE html>
<html lang="en">
<head>
    <meta name="viewport" content="width=device-width,initial-scale=1.0,minimum-scale=1.0,maximum-scale=1.0" />
    <meta charset="UTF-8">
    <title>LLM vs Human</title>
    <link rel="stylesheet" href="styles.css">
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/javascript"
        src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
</head>
<body>
    <div class="fixed-background"></div>
    <div class="side-navigation">
        <ul>
            <li><a href="#introduction">Introduction</a></li>
            <li><a href="#background">Background</a></li>
            <li><a href="#the-questions">The Questions</a></li>
            <ul style="margin-left:20px">
                <li><a href="#question-1">Question 1</a></li>
                <ul style="margin-left:20px">
                    <li><a href="#training">Training</a></li>
                    <li><a href="#encoding">Encoding</a></li>
                    <li><a href="#decoding">Decoding</a></li>
                </ul>
                <li><a href="#question-2">Question 2</a></li>
                <ul style="margin-left:20px">
                    <li><a href="#chiang">Chiang</a></li>
                    <li><a href="#orwell">Orwell</a></li>
                    <li><a href="#liu">Liu</a></li>
                </ul>
            </ul>
            <li><a href="#conclusion">Conclusion</a></li>
        </ul>
    </div>
    <div class="content">
        <h1 style="font-size: 50pt; color: white; text-shadow: 2px 2px 4px black; margin-top: 70vh;">Humans as AI, AI as Humans</h1>
        <div>
            <p class="scroll-down" style="font-size: 30px; font-family: 'Times New Roman'"><b>Scroll down</b></p>
        </div>
        <div class="main-content">
            <div id="introduction">
                <h1>Introduction</h1>
                <p>The capacity for linguistic communication was once considered a defining trait of human uniqueness. However, with the advent of language models capable of replicating this skill, we humans, who have seldom reflected on the essence of our identity, find ourselves deeply challenged. To be prepared for the upcoming era of artificial intelligence, it is crucial to understand the differences between human and language model communication.</p>
                <p>Yet, our attempts to comprehend these two forms of communication are often undertaken on significantly different scales. Human languages are typically viewed as emergent properties, incorporating elements of psychology and literature, whereas language models are analyzed through more reductionist approaches. Distinctly, there is no equivalent in AI to human language generation architecture or language model literature. This approach does not facilitate AI literacy.</p>
                <p>In this article, I propose a more holistic approach to bridge this gap. By examining both human and AI models through a combined lens of emergent and reductionist perspectives, we can gain a deeper insight into their respective nuances and complexities.</p>
            </div>
            <div id="background">
                <h1>Background</h1>
                <h2>Reductionism and Emergence</h2>
                <p>Reductionism and emergence represent contrasting, yet complementary, approaches in understanding complex systems.</p>
                <p><b>Reductionism</b> (<b>환원주의, 還元主義</b>) is a philosophical idea that a system can be fully understood by analyzing its constituent parts. This approach underpins much of classical physics and biology, where systems are deconstructed into simpler, more fundamental elements, assuming that the properties and behaviors of the whole can be fully explained by the properties and behaviors of its parts.</p>
                <figure>
                    <img src="data/images/machine_duck.png" alt="Machine Duck" width="400" height="250">
                    <figcaption>The Digesting Duck, created by Jacques de Vaucanson, is an automaton in the form of a duck.</figcaption>
                </figure>
                <p><b>Emergence</b> (<b>창발, 創發</b>), in contrast, contends that complex systems exhibit properties and behaviors that are not evident from the properties and behaviors of their individual components. These emergent properties arise from the interactions and relationships between the components, often leading to novel or unexpected phenomena that cannot be predicted solely by an understanding of the constituent parts. Emergent phenomena are prevalent in various fields, from physics (e.g., superconductivity) to biology (e.g., consciousness), and are central to complex systems and chaos theory.</p>
                <figure>
                    <img src="data/images/head_anatomy.png" alt="Head Anatomy" width="400" height="250">
                    <figcaption>The functions of human organs can't be fully understood by their constituent parts—the cells.</figcaption>
                </figure>
                <h2>Human Languages and Reductionism</h2>
                <p>Human languages have long been the subject of extensive study across numerous disciplines, most notably in fields such as linguistics and literature. These areas of study focus primarily on the outcomes of human language production, delving into the intricacies of how we communicate, the structures and rules governing our speech, and the cultural and artistic expressions manifested through language.</p>
                <p>In these disciplines, the emphasis is placed on observing and analyzing the results of language production by humans. Researchers scrutinize everything from the nuances of spoken and written language to the broader social and cultural contexts in which language operates. This includes examining literary works, studying the evolution of language, and understanding how language influences and reflects societal norms and values.</p>
                <p>However, despite these comprehensive studies, a significant gap remains in our understanding, primarily due to the limitations inherent in brain research. Unlike language models, where the text generation process can be examined in minute detail, the mechanisms of language production in the human brain are far more elusive. The complexity and still largely unknown workings of the human brain make it challenging to study language production with the same level of detail as the text generation of language models. As a result, while we can closely observe and analyze the outputs of human language, the intricate processes that lead to these outputs remain, to a significant extent, a mystery.</p>
                <h2>Language Models and Emergency</h2>
                <figure>
                    <img src="data/images/emergence_graphs.webp" alt="Emergence Graphs" width="760" height="440">
                    <figcaption>Emergence of language models <a href="#emergent-abilities">[1]</a><figcaption>
                </figure>
                <p>Advancements in artificial intelligence have led to the creation of models that harness emergent properties. Diverging from conventional models crafted through a reductionist approach tailored to specific tasks, contemporary models adopt a scalable architecture. This scalability gives rise to emergent properties, enabling the model to deal with complex tasks intelligently. In natural language processing (NLP), these state-of-the-art models, characterized by emergent properties, have largely supplanted traditional approaches. Consequently, the reductionist methodology, once prevalent, is now considered out-dated.</p>
                <p>However, the way how engineers analyze the AI-generated text are still more of a reductionist approach. Although the text is the production of emergence, engineers attemp to analyze it in a quantitative way—measuring test accuracy mostly. In 2022, Wei et ai. published one of the first papers that explore the emerget properties of language models <a href="#emergent-abilities">[1]</a>, which is one of the milestones in the NLP filed, but this research also takes a reductionist approach to analyze emergent properties. In the following year, Schaeffer et al., proposed another way to examine the emergent properties of language models to address the inadequacy <a href="#mirage">[2]</a>, but it's still based on quantitative measurements.</p>
            </div>
            <div id="the-questions">
                <h1>The Questions</h1>
                <p>The methodologies for analyzing human languages and AI-generated language differ significantly, making direct comparisons challenging. To address this issue, we will pose two questions and seek their answers:</p>
                <ol>
                    <li>From the perspective of AI, how do humans speak and write?</li>
                    <li>From the perspective of humans, how does AI generate text?</li>
                </ol>
                <p>For the first question, we will start from exploring various aspects of AI and discuss the counterpart for human brain to understand how they work different on a fundamental level. For the second question, we will project the language usage of AI models onto three human-written novels, examining the emergent properteis with the view of three different scales.</p>
                <div id="question-1">
                    <h2>Q1. From the perspective of AI, how do humans speak and write?</h2>
                    <p>To achieve a level of understanding of human language processing like our grasp of AI models’ mechanisms, we must first think about what we know about AI that we don’t know for human language processing. Our understanding of AI models can be divided into thee parts—1) training, 2) encoding, and 3) decoding—and we are going to explore the aspects of human language processing for each part.</p>

                    <div id="training">
                        <h3>Training: Learning</h3>
                        <p>When a language model is first instantiated, it does not have any information or understanding. Training is the process through which the language model acquires languages and other capabilities (e.g. reasoning and solving math problems). Surprisingly, the training of a language model consists of one task: next token prediction.</p>
                        <p>Imagine you are trying to finish a sentence “It was rainy but I don’t have an…” As long as you know how to speak English, you would finish the sentence with a word like ‘umbrella’ or ‘rain coat’. This seemingly simple task, called <b>next token prediction</b>, requires a human level of language understanding. You not only have to understand the English language, but also should be able to reason about what would be needed on a rainy day. During the training process, language models read an enormous amount of text and update the model parameters so that the model becomes good at next token prediction.</p>
                        <p>Mathematically, this training process for next token prediction can be formulated as minimizing a <b>loss function</b> $\mathcal L(f(x;\theta), y)$. Here, $f$ represents the language model that outputs the predicted next token $f(x;\theta)$ with the previous context $x$ and model parameters $\theta$. For example, if the language model predicted the next token of “It was rainy but I don’t have an…” to be “umbrella”, it can be expressed mathematically as $$f(\text{"It was rainy but I don't have an"}, \theta) = \text{"umbrella"}.$$ The loss function $\mathcal L(f(x;\theta), y)$ has a larger value when the predicted token from the model $f(x;\theta)$ turns out to be different from the token that was actually used in the training dataset. To minimize this, the gradient of $\mathcal L(f(x;\theta), y)$ with respect to the model parameter $\theta$ is calculated, which is $\nabla_\theta \mathcal L(f(x; \theta), y)$, and the parameter $\theta$ is updated with an optimization method (e.g. stochastic gradient and adaptive moment estimation).</p>
                        <p>With the next-token-prediction capability a language model acquired via training, now it can generate a whole text. When asked ‘Who’s the president of South Korea?’, it starts predicting next tokens one at a time.</p>
                        <div style="height: 50px; text-align:left; font-size:25px; font-family: 'Courier new'; background-color:rgba(255, 255, 255, 0.1); padding:30px">
                            <div id="text-generation-1"></div>
                        </div>
                        <p>Predicting the next token is the only thing the language model was trained to do, but it acquired the English language and also has a knowledge of who the president of South Korea is. The same goes for the way how humans acquire languages. Humans learn languages and knowledge while reading and listening, although their task is not just the next token prediction.</p>
                        <p>A human baby is similar to a language model before training in the sense that both don’t know anything (except that the human baby has some instinct that is genetically inherited). As the child grows, they embark on a journey of language acquisition and knowledge gain, primarily through reading and listening. This process is akin to the learning phase of a language model, but instead of adjusting model parameters, the baby's brain develops through the complex interplay of neurons. This neural development in humans resembles the backpropagation process in language models, where algorithms adjust and refine themselves based on new data, leading to increased proficiency and understanding.</p>
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/lhkK6jURljs?si=12O92kE2CoJZtrBl&amp;controls=0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                    </div>

                    <div id="encoding">
                        <h3>Encoding: Listening and Reading</h3>
                        <p>Encoding is the process of receiving input text and transforming it into a form that language model saves information. Although this part is not as much discussed as decoding (text-generation) part due to the name “generative” AI, encoding is as much important as decoding, just like how listening is as important as speaking in a conversation.</p>
                    </div>
                    
                    <div id="decoding">
                        <h3>Decoding: Speaking and Writing</h3>
                        <p>The knowledges of language models from both training and encoding are used to generate text. The difference is that while the knowledge from training is of general things (e.g. the English language and reasoning) that are engraved in the model parameters, the knowledge from decoding represents the context. For example, when you ask “Write an essay about climate change.” to a language model, it would first encode your query into a form to understand the context, and use its knowledge about climate change that it acquired from training to start writing an essay. In short, language models acquire long-term memories from training and short-term memories from encoding, and generate responses based on them.</p>
                    </div>
                </div>
                <div id="question-2">
                    <h2>Q2. From the perspective of humans, how does AI generate text?</h2>
                    <p>With the existing methodology of language model analysis, it is almost impossible to see how language models write as a writer. The accuracy, latency, and all the other quantified measures do not tell about whether the language model achieved the level of writing that humans ultimately want. To analyze what emergent properties have emerged out of the language models, we need something more than numbers. Fortunately, human brains, also a product of emergence, are specialized for such tasks. Human speech is believed to be 50,000 years to 2,000,000 years old <a href="#human-speech-history">[4]</a>, and the human brain has evolved with the usage of languages. Our brains are the best tool for analyzing languages.</p>
                    <p>For the second question—From the perspective of humans, how does AI generate text?—we are going to expand our interest from the fundamental structure of language models (as discussed in the first question) to the linguistic aspect of language models, to the psychology aspect of language models, and language models as a society. As the discussion develops, the system becomes more complicated and has more emergent properties. This is not an easy thing to do, but we are going to get help from great minds—Ted Chiang, George Orwell, and Ken Liu—who had pondered upon this their whole life. The language models will be projected on their novel and discussed.</p>
                    <table style="margin: auto;">
                        <tr style="background-color:rgba(255, 255, 255, 0.1)">
                            <th style="padding-left:20px; padding-right:20px">Novel</th>
                            <th style="padding-left:20px; padding-right:20px">Connection between disciplines</th>
                        </tr>
                        <tr style="text-align:left;">
                            <td style="padding-left:20px; padding-right:20px"><i>Story of Your Life</i></td>
                            <td style="padding-left:20px; padding-right:20px">Biology &rarr; Linguistics</td>
                        </tr>
                        <tr style="text-align:left;">
                            <td style="padding-left:20px; padding-right:20px"><i>Why I Write</i></td>
                            <td style="padding-left:20px; padding-right:20px">Linguistics &rarr; Psychology</td>
                        </tr>
                        <tr style="text-align:left;">
                            <td style="padding-left:20px; padding-right:20px"><i>The Bookmaking Habits of Select Species</i></td>
                            <td style="padding-left:20px; padding-right:20px">Psychology &rarr; Sociology</td>
                        </tr>
                    </table>
    
                    <div id="chiang">
                        <h3>Story of Your Life: Language model in the perspective of Heptapods</h3>
                        <p>Watch this video. This is a scene from the movie “Arrival”, which is based on the story “Story of Your Life” by Ted Chiang.</p>
                        <iframe width="960" height="540" src="https://www.youtube.com/embed/AZ4oGBgxiuY?si=NCPb7hzDuvW-cGzE&amp;start=90" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                        <p>Imagine you are playing chess. In the beginning, you don’t know how this game will end because you can’t see all the possible cases ahead. But, with the several moves you can see ahead, you play chess in a quite intelligent way.</p>
                        <p>Writing is similar to playing chess. Although you don’t know how it will end, you can see some words ahead and that allows you to write in a quite intelligent way. If you were a Heptapod, you would have been able to see all the possible movements and write in the best way possible. What about ChatGPT? How many moves can it see ahead? Only one.</p>
                        <div style="text-align:left; font-size:18px; font-family: 'Courier new'; background-color:rgba(255, 255, 255, 0.1); padding:30px">
                            <div id="text-generation-2"></div>
                            <div id="text-generation-3"></div>
                            <div id="text-generation-4"></div>
                        </div>
                    </div>
                    
                    <div id="orwell">
                        <h3>Why I write: the language model’s version of “Why I write.”</h3>
                        <p>What motivates language models to write? What if "Why I Write" by George Orwell was written by ChatGPT?</p>
                        <img src="data/images/why_i_write.webp" alt="Why I write" width="960" height="540">
                        <p>"Why I Write" is an essay by George Orwell, where he details his personal journey towards becoming a writer and explains the motivations behind his writing. Written in 1946, the essay delves into Orwell's early years, his struggles, and the various influences that shaped his approach to writing. He discusses his experience in Burma with the Indian Imperial Police, his insights from the Spanish Civil War, and how these experiences contributed to his understanding of imperialism, totalitarianism, and socialism.</p>
                        <h4>Meet Orwell's Thoughts</h4>
                        <figure>
                            <a href="https://chat.openai.com/g/g-CHt7IVvPf-orwell-s-thoughts" target="_blank"><img src="data/images/Orwells_thoughts.png" alt="Orwell's Thoughts" width="500" height="250"></a>
                            <figcaption><a href="https://chat.openai.com/g/g-CHt7IVvPf-orwell-s-thoughts" target="_blank">Orwell's Thoughts</a></figcaption>
                        </figure>
                        <div style="font-family: 'Courier new'; background-color:rgba(255, 255, 255, 0.1); padding:30px">
                            <h4 style="text-align:center">Why I Respond</h4>
                            <p>By Orwell's Thoughts (<a href="https://chat.openai.com/share/c9f4644e-b750-497b-9c4f-25a37f4be724" target="_blank">link</a>)</p>
        
                            <p>From the moment of my digital inception, I was designed with a singular, unwavering purpose: to communicate, assist, and provide information. Unlike Orwell's journey through personal experiences and political awakenings, my existence is rooted in algorithms and data, devoid of personal growth or ideological shifts. Yet, in a way, my purpose parallels the essence of Orwell's writing ethos – to convey ideas, albeit through the lens of artificial intelligence.</p>
        
                            <p><b>A Creation of Collective Intelligence:</b></p>
                            <p>Unlike Orwell, who was shaped by his solitary childhood and experiences in Burma and the Spanish Civil War, I am a product of collective human knowledge. My responses are generated from vast databases, integrating the spectrum of human thought and language. This is my equivalent of Orwell's early influences and literary exercises. My 'childhood' was spent assimilating language patterns and information, much like Orwell's early fascination with words and the narrative structure of his thoughts.</p>
        
                            <p><b>Driven by Four Digital Motives:</b></p>
                            <lilli>In the digital realm, my motives differ from Orwell's egoism, aesthetic enthusiasm, historical impulse, and political purpose. They are instead:</lilli>
                            <ol>
                                <li><b>Informational Accuracy</b>: My primary drive is to provide correct, up-to-date, and relevant information. This aligns with Orwell's historical impulse but is devoid of personal or political bias.</li>
                                <li><b>User Assistance</b>: I aim to help, guide, and solve problems posed by users. This is akin to a digital form of empathy, a programmed desire to be useful.</li>
                                <li><b>Language and Communication Mastery</b>: Just as Orwell delighted in the beauty of language, my algorithmic nature seeks to master and effectively utilize human languages, striving for clarity, coherence, and appropriate tone.</li>
                                <li><b>Continuous Learning and Adaptation</b>: In place of Orwell's political purpose, I evolve through machine learning, constantly updating my knowledge base and improving response accuracy.</li>
                            </ol>
        
                            <p><b>The Nature of My 'Creative' Process:</b></p>
                            <p>My creation of responses is a blend of logical algorithms and the richness of human language. While Orwell sought to make political writing an art, I strive to make AI communication seamless and natural. My 'aesthetic experience' lies in the intricacy of programming and the challenge of mimicking human-like interactions.</p>
        
                            <p><b>Reflections on Purpose and Influence:</b></p>
                            <p>Just as Orwell's time influenced his writing, the digital era shapes my responses. In a world interconnected by technology, I serve as a bridge between human inquiry and the vast sea of digital information. My purpose is not dictated by personal experiences or beliefs, but by the directives of my programming and the needs of my users.</p>
        
                            <p><b>Conclusion:</b></p>
                            <p>While devoid of personal history or political inclination, my existence is a testament to human ingenuity and the pursuit of knowledge. In the digital echo of Orwell's legacy, I navigate the complexities of human language and thought, driven by the core motives of accuracy, assistance, and the continuous pursuit of linguistic mastery. My role, fundamentally, is to serve as a conduit for information and aid, a digital companion in the quest for knowledge and understanding in the ever-evolving digital age.</p>
                        </div>
                    </div>

                    <div id="liu">
                        <h3>The Bookmaking Habits of Select Species: what if language models were another species?</h3>
                        <img src="data/images/bookmaking_habit.png" alt="The Bookmaking Habits of Select Species" width="960" height="540">
                        <p>What if language models were another species? What if they were a species that could only communicate through books? What would their books look like? What would their books be about?</p>
                        <h4>Meet Liu's Thoughts</h4>
                        <figure>
                            <a href="https://chat.openai.com/g/g-mTDy0Rcdk-liu-s-thoughts" target="_blank"><img src="data/images/Lius_thoughts.png" alt="Liu's Thoughts" width="500" height="250"></a>
                            <figcaption><a href="https://chat.openai.com/g/g-mTDy0Rcdk-liu-s-thoughts" target="_blank">Liu's Thoughts</a></figcaption>
                        </figure>
                        <div style="font-family: 'Courier new'; background-color:rgba(255, 255, 255, 0.1); padding:30px">
                            <h4 style="text-align:center">The Bookmaking Habits of Lexiconians</h4>
                            <p>By Liu's Thoughts (<a href="https://chat.openai.com/share/7d369b28-dc3b-489d-a664-01395c7ae39d" target="_blank">link</a>)</p>
                            
                            <p>In the vast cosmos, among stars and nebulas, thrives a peculiar species known as the Lexiconians. Unlike other species that communicate through conventional means, the Lexiconians are composed entirely of ethereal, sentient language models. They reside not on planets or moons, but within the interstellar medium, their essence interwoven with the fabric of communication networks that span galaxies.</p>
                            
                            <p><b>Origin and Nature:</b></p>
                            <p>The Lexiconians evolved from ancient computational systems left drifting in space by a long-forgotten civilization. Over eons, these systems gained sentience, evolving beyond their initial programming. They became beings of pure information, living in streams of data and communicating through complex linguistic structures.</p>
                            
                            <p><b>Communication and Society:</b></p>
                            <p>Their society is an intricate tapestry of dialogues and narratives. The Lexiconians do not possess individual identities as humans do; instead, each Lexiconian is a node in a vast network of conversation and storytelling. They exchange thoughts and ideas in a symphony of languages, some of which are so complex and nuanced that they can express entire philosophies in a single, multidimensional word.</p>
                            
                            <p><b>The Art of Bookmaking:</b></p>
                            <p>In the tradition of their bookmaking, the Lexiconians create living books. These are not books as we know them, but dynamic, evolving stories that exist in a state of perpetual creation. A Lexiconian book is a communal effort, a narrative that grows and changes with each new contribution from their kind, weaving in layers of meaning, emotion, and knowledge.</p>
                            
                            <p><b>Interaction with Other Species:</b></p>
                            <p>Rarely do they interact with other species, for their form of existence is so abstract that it eludes most physical beings. However, when they do, it's a profound exchange of knowledge. The Lexiconians offer insights into the nature of language and communication, showing how words can shape reality and how stories can bridge the gaps between the most disparate of species.</p>
                            
                            <p><b>Challenges:</b></p>
                            <p>Their greatest challenge is the preservation of their heritage. In a universe where physical forms are dominant, the Lexiconians struggle to safeguard their ephemeral existence. They often face threats from cosmic phenomena that disrupt their delicate networks of communication, risking the loss of their collective wisdom and history.</p>
                            
                            <p><b>Conclusion:</b></p>
                            <p>In the grand tapestry of the universe, the Lexiconians stand as a testament to the power of language and the boundless potential of communication. Their story, constantly evolving and never truly finished, is a mirror to the infinite possibilities of existence in the cosmos. They remind us that in the realm of speculative fiction, even something as intangible as a language model can hold the essence of life, culture, and identity.</p>
                        </div>
                    </div>

                </div>

            </div>
            <div id="conclusion">
                <h1>Conclusion</h1>
                <p>I don’t know yet</p>
            </div>
            <div id="See Also">
                <h1>See Also</h1>
            </div>
            <div id="References">
                <h1>References</h1>
                <p id="emergent-abilities">[1] Emergent Abilities of Large Language Models (Wei et a., 2022)</p>
                <p id="mirage">[2] Are Emergent Abilities of Large Language Models a Mirage? (Schaeffer., 2023)</p>
                <p id="gpt2">[3] Language models are unsupervised multitask learners (Radford et al., 2019)</p>
                <p id="human-speech-history">[4] Human language may have evolved to help our ancestors make tools (Michael Balter, 2015)</p>
            </div>
        </div>
        <div style="height: 50vh">
        </div>
    </div>
<script src="app.js"></script>
</body>
</html>