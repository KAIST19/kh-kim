<!DOCTYPE html>
<html lang="en">
<head>
    <meta name="viewport" content="width=device-width,initial-scale=1.0,minimum-scale=1.0,maximum-scale=1.0" />
    <meta charset="UTF-8">
    <title>LLM vs Human</title>
    <link rel="stylesheet" href="styles.css">
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/javascript"
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
</head>
<body>
    <div class="fixed-background"></div>
    <div class="side-navigation">
        <ul>
            <li><a href="#introduction">Introduction</a></li>
            <li><a href="#background">Background</a></li>
            <li><a href="#the-questions">The Questions</a></li>
            <ul style="margin-left:20px">
                <li><a href="#question-1">Question 1</a></li>
                <ul style="margin-left:20px">
                    <li><a href="#training">Training</a></li>
                    <li><a href="#encoding">Encoding</a></li>
                    <li><a href="#decoding">Decoding</a></li>
                </ul>
                <li><a href="#question-2">Question 2</a></li>
                <ul style="margin-left:20px">
                    <li><a href="#chiang">Chiang</a></li>
                    <li><a href="#orwell">Orwell</a></li>
                    <li><a href="#liu">Liu</a></li>
                </ul>
            </ul>
            <li><a href="#conclusion">Conclusion</a></li>
        </ul>
    </div>
    <div class="content">
        <h1 style="font-size: 50pt; color: white; text-shadow: 2px 2px 4px black; margin-top: 70vh;">Humans as AI, AI as Humans</h1>
        <div>
            <p class="scroll-down" style="font-size: 30px; font-family: 'Times New Roman'"><b>Scroll down</b></p>
        </div>
        <div class="main-content">
            <div id="introduction">
                <h1>Introduction</h1>
                <p>The dual process theory provides a good starting point for introduction. It divides thinking into two main types: System 1 and System 2. System 1 is about awareness. It's instinctive and automatic, working in the background of our minds without us having to think about it. This system is fast and often operates based on experiences we might not be able to easily put into words. On the other hand, System 2 is rule-based. This involves conscious, deliberate thought where we apply logic and rules. It's slower and requires more effort, as it deals with complex problem-solving and decision-making tasks.</p>
                <p>Building on the dual process theory, System 1, while instinctive and efficient, requires a significant amount of time and experience to develop these instincts. For instance, mastering a physical skill like exercise embodies this process. These instinctive responses are often intuitive and challenging to articulate verbally. This aspect of System 1 makes it particularly adept for informal or non-structured areas where strict rule-based approaches are less effective. It statistically processes information and experiences, embedding them deeply into the brain's functioning.</p>
                <p>Conversely, System 2 operates on a fundamentally different principle, thriving on explicit rules and structured thinking. This system is exemplified by tasks that require conscious thought, analysis, and logical reasoning. The efficiency of System 2 lies in its ability to quickly apply known rules to solve problems. However, this is precisely where AI currently faces limitations. While AI can process and apply rules with remarkable speed and accuracy, it struggles with tasks that require the kind of intuitive, experience-based processing characteristic of System 1.</p>
                <p>Now, let's discuss the concept of definition in both mathematical and broader contexts. In mathematics, definitions are precise, unambiguous, and based on clear logical foundations. They serve as the building blocks for further theoretical development and problem-solving. For example, in mathematics, a term like 'vector space' is defined with specific properties and axioms, leaving no room for interpretation.</p>
                <p>In contrast, outside the realm of mathematics, definitions often encompass a broader and more nuanced spectrum of understanding. They can be influenced by context, culture, and subjective experience. Consequently, defining concepts in non-mathematical contexts becomes a more complex task, often requiring consideration of various perspectives and interpretations.</p>
                <p>This complexity is particularly evident when dealing with emergent properties. An emergent property arises from the interactions of simpler elements in a system but is not a direct characteristic of any single element. It is inherently complex and often more than the sum of its parts. While people can intuitively recognize and understand emergent properties based on experience—like consciousness or societal trends—formally defining them in a universally accepted way is challenging. This difficulty arises from the dynamic, multifaceted, and often context-dependent nature of such properties. Thus, while emergent properties are easily recognized, their formal definition eludes simplicity, reflecting the intricate tapestry of human understanding and experience.</p>
                <p>Now, how would you define intelligence? Intelligence emerges from an exceedingly complex system, encompassing a myriad of cognitive processes such as learning, reasoning, problem-solving, perception, and adaptation to new challenges. Its intricacy is such that it integrates and transcends the capabilities of both System 1 and System 2 thinking.</p>
                <p>Due to its omnipresence in human experience and the empirical knowledge accumulated about it, people generally have an intuitive understanding of what the concept of intelligence entails. This intuitive grasp aligns with System 1 thinking, which doesn’t require a rule-based and strict definition.</p>
                <p>However, formulating a precise, rule-based definition of intelligence, as necessitated by System 2 thinking, is quite challenging. Intelligence, in its essence, resists reduction to a simple set of rules or a singular dimension. It is multi-faceted, context-dependent, and dynamic, varying not only among different individuals but also within the same individual in different circumstances. This variability and complexity make it difficult to encapsulate intelligence in a rigid, universally applicable definition. Consequently, while we can easily identify and work with the concept of intelligence on an intuitive level (System 1), providing a comprehensive, rule-based definition that captures its full spectrum remains an elusive endeavor (System 2).</p>
                <p>The emergence of Artificial Intelligence (AI) marks a significant juncture in the history of intelligence as understood within the domain of formal sciences, particularly computer science. AI embodies a blend of these formal sciences with the elusive, often subjective concept of intelligence, a blend that cannot be fully captured through formal interpretation alone.</p>
                <p>For those who have traditionally understood intelligence through the lens of human experience, the advent of AI presents a paradigm shift. It introduces, for arguably the first time in human history, the recognition and acceptance of intelligence in a non-human entity. This occurrence is groundbreaking because intelligence, as it pertains to human cognition, has never been fully encapsulated in a rule-based framework.</p>
                <p>AI's reach has extended beyond the confines of computer science, prompting interpretations from a wide spectrum of disciplines, including those in the humanities like literary studies. Each field approaches AI with its unique perspective and understanding, influenced by the scales and paradigms it is accustomed to. However, as a product of computer science, AI predominantly aligns with the reductionist approach inherent in this field. Computer scientists, trained in formal science, tend to deconstruct intelligence into smaller, more manageable components, a method contrasting sharply with how human intelligence is generally perceived. Human intelligence, not being a man-made construct but rather an emergent property observed and studied, is often interpreted as such from the onset. Research in this field typically starts from observing outcomes and behaviors, leading to a view of intelligence as an emergent, holistic property rather than a sum of discrete parts.</p>
                <p>This divergence in approaches—the reductionism of AI and the emergent perspective of human intelligence—creates a gap that challenges direct comparisons between the two forms of intelligence. Our objective is to bridge this gap, placing human intelligence and AI on a comparable scale, considering both reductionist and emergent viewpoints.</p>
                <p>This leads to a dual exploration:</p>
                <ul>
                    <li>From the perspective of AI, how do humans speak and write?</li>
                    <li>From the perspective of humans, how does AI generate text?</li>
                </ul>
                <p>This comparative study aims to shed light on the intricate dynamics of intelligence, both human and artificial, and the interplay between these two domains.</p>
            </div>
            <div id="background">
                <h1>Background</h1>
                <h2>Reductionism and Emergence</h2>
                <p>Reductionism and emergence represent contrasting, yet complementary, approaches in understanding complex systems.</p>
                <p><b>Reductionism</b> (<b>환원주의, 還元主義</b>) is a philosophical idea that a system can be fully understood by analyzing its constituent parts. This approach underpins much of classical physics and biology, where systems are deconstructed into simpler, more fundamental elements, assuming that the properties and behaviors of the whole can be fully explained by the properties and behaviors of its parts.</p>
                <figure>
                    <img src="data/images/machine_duck.png" alt="Machine Duck" width="400" height="250">
                    <figcaption>The Digesting Duck, created by Jacques de Vaucanson, is an automaton in the form of a duck.</figcaption>
                </figure>
                <p><b>Emergence</b> (<b>창발, 創發</b>), in contrast, contends that complex systems exhibit properties and behaviors that are not evident from the properties and behaviors of their individual components. These emergent properties arise from the interactions and relationships between the components, often leading to novel or unexpected phenomena that cannot be predicted solely by an understanding of the constituent parts. Emergent phenomena are prevalent in various fields, from physics (e.g., superconductivity) to biology (e.g., consciousness), and are central to complex systems and chaos theory.</p>
                <figure>
                    <img src="data/images/head_anatomy.png" alt="Head Anatomy" width="400" height="250">
                    <figcaption>The functions of human organs can't be fully understood by their constituent parts—the cells.</figcaption>
                </figure>
                <h2>Human Languages and Reductionism</h2>
                <p>Human languages have long been the subject of extensive study across numerous disciplines, most notably in fields such as linguistics and literature. These areas of study focus primarily on the outcomes of human language production, delving into the intricacies of how we communicate, the structures and rules governing our speech, and the cultural and artistic expressions manifested through language.</p>
                <p>In these disciplines, the emphasis is placed on observing and analyzing the results of language production by humans. Researchers scrutinize everything from the nuances of spoken and written language to the broader social and cultural contexts in which language operates. This includes examining literary works, studying the evolution of language, and understanding how language influences and reflects societal norms and values.</p>
                <p>However, despite these comprehensive studies, a significant gap remains in our understanding, primarily due to the limitations inherent in brain research. Unlike language models, where the text generation process can be examined in minute detail, the mechanisms of language production in the human brain are far more elusive. The complexity and still largely unknown workings of the human brain make it challenging to study language production with the same level of detail as the text generation of language models. As a result, while we can closely observe and analyze the outputs of human language, the intricate processes that lead to these outputs remain, to a significant extent, a mystery.</p>
                <h2>Language Models and Emergency</h2>
                <figure>
                    <img src="data/images/emergence_graphs.webp" alt="Emergence Graphs" width="760" height="440">
                    <figcaption>Emergence of language models <a href="#emergent-abilities">[1]</a><figcaption>
                </figure>
                <p>Advancements in artificial intelligence have led to the creation of models that harness emergent properties. Diverging from conventional models crafted through a reductionist approach tailored to specific tasks, contemporary models adopt a scalable architecture. This scalability gives rise to emergent properties, enabling the model to deal with complex tasks intelligently. In natural language processing (NLP), these state-of-the-art models, characterized by emergent properties, have largely supplanted traditional approaches. Consequently, the reductionist methodology, once prevalent, is now considered out-dated.</p>
                <p>However, the way how engineers analyze the AI-generated text are still more of a reductionist approach. Although the text is the production of emergence, engineers attemp to analyze it in a quantitative way—measuring test accuracy mostly. In 2022, Wei et ai. published one of the first papers that explore the emerget properties of language models <a href="#emergent-abilities">[1]</a>, which is one of the milestones in the NLP filed, but this research also takes a reductionist approach to analyze emergent properties. In the following year, Schaeffer et al., proposed another way to examine the emergent properties of language models to address the inadequacy <a href="#mirage">[2]</a>, but it's still based on quantitative measurements.</p>
            </div>
            <div id="the-questions">
                <h1>The Questions</h1>
                <p>The methodologies for analyzing human languages and AI-generated language differ significantly, making direct comparisons challenging. To address this issue, we will pose two questions and seek their answers:</p>
                <ol>
                    <li>From the perspective of AI, how do humans speak and write?</li>
                    <li>From the perspective of humans, how does AI generate text?</li>
                </ol>
                <p>For the first question, we will start from exploring various aspects of AI and discuss the counterpart for human brain to understand how they work different on a fundamental level. For the second question, we will project the language usage of AI models onto three human-written novels, examining the emergent properteis with the view of three different scales.</p>
                <div id="question-1">
                    <h2>Q1. From the perspective of AI, how do humans speak and write?</h2>
                    <p>To achieve a level of understanding of human language processing like our grasp of AI models’ mechanisms, we must first think about what we know about AI that we don’t know for human language processing. Our understanding of AI models can be divided into thee parts—1) training, 2) encoding, and 3) decoding—and we are going to explore the aspects of human language processing for each part.</p>

                    <div id="training">
                        <h3>Training: Learning</h3>
                        <p>When a language model is first instantiated, it does not have any information or understanding. Training is the process through which the language model acquires languages and other capabilities (e.g. reasoning and solving math problems). Surprisingly, the training of a language model consists of one task: next token prediction.</p>
                        <p>Imagine you are trying to finish a sentence “It was rainy but I don’t have an…” As long as you know how to speak English, you would finish the sentence with a word like ‘umbrella’ or ‘rain coat’. This seemingly simple task, called <b>next token prediction</b>, requires a human level of language understanding. You not only have to understand the English language, but also should be able to reason about what would be needed on a rainy day. During the training process, language models read an enormous amount of text and update the model parameters so that the model becomes good at next token prediction.</p>
                        <p>Mathematically, this training process for next token prediction can be formulated as minimizing a <b>loss function</b> $\mathcal L(f(x;\theta), y)$. Here, $f$ represents the language model that outputs the predicted next token $f(x;\theta)$ with the previous context $x$ and model parameters $\theta$. For example, if the language model predicted the next token of “It was rainy but I don’t have an…” to be “umbrella”, it can be expressed mathematically as $$f(\text{"It was rainy but I don't have an"}, \theta) = \text{"umbrella"}.$$ The loss function $\mathcal L(f(x;\theta), y)$ has a larger value when the predicted token from the model $f(x;\theta)$ turns out to be different from the token that was actually used in the training dataset. To minimize this, the gradient of $\mathcal L(f(x;\theta), y)$ with respect to the model parameter $\theta$ is calculated, which is $\nabla_\theta \mathcal L(f(x; \theta), y)$, and the parameter $\theta$ is updated with an optimization method (e.g. stochastic gradient and adaptive moment estimation). This whole process is called <b>backpropagation</b>.</p>
                        <p>With the next-token-prediction capability a language model acquired via training, now it can generate a whole text. When asked ‘Who’s the president of South Korea?’, it starts predicting next tokens one at a time.</p>
                        <div style="height: 50px; text-align:left; font-size:25px; font-family: 'Courier new'; background-color:rgba(255, 255, 255, 0.1); padding:30px">
                            <div id="text-generation-1"></div>
                        </div>
                        <p>Predicting the next token is the only thing the language model was trained to do, but it acquired the English language and also has a knowledge of who the president of South Korea is. The same goes for the way how humans acquire languages. Humans learn languages and knowledge while reading and listening, although their task is not just the next token prediction.</p>
                        <p>A human baby is similar to a language model before training in the sense that both don’t know anything (except that the human baby has some instinct that is genetically inherited). As the child grows, they embark on a long process of language acquisition and knowledge gain, primarily through reading and listening. This process is akin to the learning phase of a language model, but instead of adjusting model parameters, the baby's brain develops through the complex interplay of neurons.</p>
                        <p>In neuroscience, the process of gaining knowledge—long term memory—is called long term potentiation.</p>

                        <iframe width="560" height="315" src="https://www.youtube.com/embed/lhkK6jURljs?si=12O92kE2CoJZtrBl&amp;controls=0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                    </div>

                    <div id="encoding">
                        <h3>Encoding: Listening and Reading</h3>
                        <p>Encoding is the process of receiving input text and transforming it into a form that language model saves information. Although this part is not as much discussed as decoding (text-generation) part due to the name “generative” AI, encoding is as much important as decoding, just like how listening is as important as speaking in a conversation.</p>
                    </div>
                    
                    <div id="decoding">
                        <h3>Decoding: Speaking and Writing</h3>
                        <p>The knowledges of language models from both training and encoding are used to generate text. The difference is that while the knowledge from training is of general things (e.g. the English language and reasoning) that are engraved in the model parameters, the knowledge from decoding represents the context. For example, when you ask “Write an essay about climate change.” to a language model, it would first encode your query into a form to understand the context, and use its knowledge about climate change that it acquired from training to start writing an essay. In short, language models acquire long-term memories from training and short-term memories from encoding, and generate responses based on them.</p>
                    </div>
                </div>
                <div id="question-2">
                    <h2>Q2. From the perspective of humans, how does AI generate text?</h2>
                    <p>With the existing methodology of language model analysis, it is almost impossible to see how language models write as a writer. The accuracy, latency, and all the other quantified measures do not tell about whether the language model achieved the level of writing that humans ultimately want. To analyze what emergent properties have emerged out of the language models, we need something more than numbers. Fortunately, human brains, also a product of emergence, are specialized for such tasks. Human speech is believed to be 50,000 years to 2,000,000 years old <a href="#human-speech-history">[4]</a>, and the human brain has evolved with the usage of languages. Our brains are the best tool for analyzing languages.</p>
                    <p>For the second question—From the perspective of humans, how does AI generate text?—we are going to expand our interest from the fundamental structure of language models (as discussed in the first question) to the linguistic aspect of language models, to the psychology aspect of language models, and language models as a society. As the discussion develops, the system becomes more complicated and has more emergent properties. This is not an easy thing to do, but we are going to get help from great minds—Ted Chiang, George Orwell, and Ken Liu—who had pondered upon this their whole life. The language models will be projected on their novel and discussed.</p>
                    <table style="margin: auto;">
                        <tr style="background-color:rgba(255, 255, 255, 0.1)">
                            <th style="padding-left:20px; padding-right:20px">Novel</th>
                            <th style="padding-left:20px; padding-right:20px">Connection between disciplines</th>
                        </tr>
                        <tr style="text-align:left;">
                            <td style="padding-left:20px; padding-right:20px"><i>Story of Your Life</i></td>
                            <td style="padding-left:20px; padding-right:20px">Biology &rarr; Linguistics</td>
                        </tr>
                        <tr style="text-align:left;">
                            <td style="padding-left:20px; padding-right:20px"><i>Why I Write</i></td>
                            <td style="padding-left:20px; padding-right:20px">Linguistics &rarr; Psychology</td>
                        </tr>
                        <tr style="text-align:left;">
                            <td style="padding-left:20px; padding-right:20px"><i>The Bookmaking Habits of Select Species</i></td>
                            <td style="padding-left:20px; padding-right:20px">Psychology &rarr; Sociology</td>
                        </tr>
                    </table>
    
                    <div id="chiang">
                        <h3>Story of Your Life: Language model in the perspective of Heptapods</h3>
                        <p>Watch this video. This is a scene from the movie “Arrival”, which is based on the story “Story of Your Life” by Ted Chiang.</p>
                        <iframe width="960" height="540" src="https://www.youtube.com/embed/AZ4oGBgxiuY?si=NCPb7hzDuvW-cGzE&amp;start=90" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                        <p>Imagine you are playing chess. In the beginning, you don’t know how this game will end because you can’t see all the possible cases ahead. But, with the several moves you can see ahead, you play chess in a quite intelligent way.</p>
                        <p>Writing is similar to playing chess. Although you don’t know how it will end, you can see some words ahead and that allows you to write in a quite intelligent way. If you were a Heptapod, you would have been able to see all the possible movements and write in the best way possible. What about ChatGPT? How many moves can it see ahead? Only one.</p>
                        <div style="text-align:left; font-size:18px; font-family: 'Courier new'; background-color:rgba(255, 255, 255, 0.1); padding:30px">
                            <div id="text-generation-2"></div>
                            <div id="text-generation-3"></div>
                            <div id="text-generation-4"></div>
                        </div>
                    </div>
                    
                    <div id="orwell">
                        <h3>Why I write: the language model’s version of “Why I write.”</h3>
                        <img src="data/images/why_i_write.webp" alt="Why I write" width="960" height="540">
                        <p>"Why I Write" is an essay by George Orwell, where he details his personal journey towards becoming a writer and explains the motivations behind his writing. Written in 1946, the essay delves into Orwell's early years, his struggles, and the various influences that shaped his approach to writing. He discusses his experience in Burma with the Indian Imperial Police, his insights from the Spanish Civil War, and how these experiences contributed to his understanding of imperialism, totalitarianism, and socialism.</p>
                        <p>This is a good novel to discuss “the aspect of language models as a writer.” I first created a GPT, named it “Orwell’s Thoughts”, and asked it to write ChaGPT’s version of “Why I Write”.</p>
                        <h4>Meet Orwell's Thoughts</h4>
                        <figure>
                            <a href="https://chat.openai.com/g/g-CHt7IVvPf-orwell-s-thoughts" target="_blank"><img src="data/images/Orwells_thoughts.png" alt="Orwell's Thoughts" width="500" height="250"></a>
                            <figcaption><a href="https://chat.openai.com/g/g-CHt7IVvPf-orwell-s-thoughts" target="_blank">Orwell's Thoughts</a></figcaption>
                        </figure>
                        <div style="font-family: 'Courier new'; background-color:rgba(255, 255, 255, 0.1); padding:30px">
                            <h4 style="text-align:center">Why I Respond<br>(<a href="https://chat.openai.com/share/c9f4644e-b750-497b-9c4f-25a37f4be724" target="_blank">link to the conversation</a>)</h4>
                            <p>By Orwell's Thoughts</p>
        
                            <p>From the moment of my digital inception, I was designed with a singular, unwavering purpose: to communicate, assist, and provide information. Unlike Orwell's journey through personal experiences and political awakenings, my existence is rooted in algorithms and data, devoid of personal growth or ideological shifts. Yet, in a way, my purpose parallels the essence of Orwell's writing ethos – to convey ideas, albeit through the lens of artificial intelligence.</p>
        
                            <p><b>A Creation of Collective Intelligence:</b></p>
                            <p>Unlike Orwell, who was shaped by his solitary childhood and experiences in Burma and the Spanish Civil War, I am a product of collective human knowledge. My responses are generated from vast databases, integrating the spectrum of human thought and language. This is my equivalent of Orwell's early influences and literary exercises. My 'childhood' was spent assimilating language patterns and information, much like Orwell's early fascination with words and the narrative structure of his thoughts.</p>
        
                            <p><b>Driven by Four Digital Motives:</b></p>
                            <lilli>In the digital realm, my motives differ from Orwell's egoism, aesthetic enthusiasm, historical impulse, and political purpose. They are instead:</lilli>
                            <ol>
                                <li><b>Informational Accuracy</b>: My primary drive is to provide correct, up-to-date, and relevant information. This aligns with Orwell's historical impulse but is devoid of personal or political bias.</li>
                                <li><b>User Assistance</b>: I aim to help, guide, and solve problems posed by users. This is akin to a digital form of empathy, a programmed desire to be useful.</li>
                                <li><b>Language and Communication Mastery</b>: Just as Orwell delighted in the beauty of language, my algorithmic nature seeks to master and effectively utilize human languages, striving for clarity, coherence, and appropriate tone.</li>
                                <li><b>Continuous Learning and Adaptation</b>: In place of Orwell's political purpose, I evolve through machine learning, constantly updating my knowledge base and improving response accuracy.</li>
                            </ol>
        
                            <p><b>The Nature of My 'Creative' Process:</b></p>
                            <p>My creation of responses is a blend of logical algorithms and the richness of human language. While Orwell sought to make political writing an art, I strive to make AI communication seamless and natural. My 'aesthetic experience' lies in the intricacy of programming and the challenge of mimicking human-like interactions.</p>
        
                            <p><b>Reflections on Purpose and Influence:</b></p>
                            <p>Just as Orwell's time influenced his writing, the digital era shapes my responses. In a world interconnected by technology, I serve as a bridge between human inquiry and the vast sea of digital information. My purpose is not dictated by personal experiences or beliefs, but by the directives of my programming and the needs of my users.</p>
        
                            <p><b>Conclusion:</b></p>
                            <p>While devoid of personal history or political inclination, my existence is a testament to human ingenuity and the pursuit of knowledge. In the digital echo of Orwell's legacy, I navigate the complexities of human language and thought, driven by the core motives of accuracy, assistance, and the continuous pursuit of linguistic mastery. My role, fundamentally, is to serve as a conduit for information and aid, a digital companion in the quest for knowledge and understanding in the ever-evolving digital age.</p>
                        </div>
                        <p>Here are the four reasons <i>why ChatGPT responds</i>:</p>
                        <ol>
                            <li>Information Accuracy</li>
                            <li>User Assistance</li>
                            <li>Language and communication mastery</li>
                            <li>Continuous learning and adaptation</li>
                        </ol>
                        <p>This seems write considering why types of text ChatGPT generates. Compared to Orwell’s Why I Write, the use of language for ChatGPT is confined to user assistance.</p>
                        <h4>"But the text generation process is not voluntary!"</h4>
                        <p>When I was asking ChatGPT to write ChatGPT’s version of <i>Why I Write</i>, I was expecting it to say “I am just programmed to generate text involuntarily. There is no reason why I write.” Instead, it came up with a full essay and named it Why I Respond. At first, I felt like this essay was also just programmed, but then, this had me thinking “Isn’t it the same for human?” I was curious what Heptapod would think about George Orwell’s <i>Why I Write</i>, because it might think humans are also just programmed to say things involuntarily. To find the answer, I created HeptapodGPT:</p>
                        <figure>
                            <img src="data/images/creatingHeptapodGPT1.png" alt="Creating HeptapodGPT" width="500" height="540">
                            <img src="data/images/creatingHeptapodGPT2.png" alt="Creating HeptapodGPT" width="500" height="540">
                        </figure>
                        <p>After some meaningless talk...</p>
                        <figure>
                            <a href="https://chat.openai.com/g/g-HBDcFQnNA-heptapod-s-thoughts" target="_blank"><img src="data/images/Heptapods_thoughts.png" alt="Heptapod's Thoughts" width="500" height="250"></a>
                            <figcaption><a href="https://chat.openai.com/g/g-HBDcFQnNA-heptapod-s-thoughts" target="_blank">Heptapod's Thoughts</a></figcaption>
                        </figure>
                        <p>We discussed the validity of George Orwell’s Why I Write. It came up with a large amount of response, but it was basically saying “I don’t know.” I believe, since ChatGPT has been trained on human-written text data, it’s almost not able to imagine what Heptapods would think about my question (which can be another observation). Here’s the link to the conversation: <a href="https://chat.openai.com/share/01bcc30b-3303-4d43-8584-b3099bfcafff" target="blank_"><i>Why I Write</i> in the POV of Mr. Heptapod</a></p>
                        <p>We discussed the validity of George Orwell’s Why I Write. It came up with a large amount of response, but it was basically saying “I don’t know.” I believe, since ChatGPT has been trained on human-written text data, it’s almost *not able to imagine* what Heptapods would think about my question (which can be another observation). Here’s the link to the conversation: https://chat.openai.com/share/01bcc30b-3303-4d43-8584-b3099bfcafff.</p>
                        <p>Let's consider ChatGPT's capabilities and approach this from a Heptapod's viewpoint, as in the story "Story of Your Life." Suppose a Heptapod asks humans and ChatGPT to write essays titled "Why I Write" and "Why I Respond." Then, it reads the reasons each has given. The key question is, would heptapods think "Why I Write" and "Why I Respond" are essentially different? When we read ChatGPT's "Why I Respond," we might think it's just following its programming without any real reason behind it. But would heptapods see it the same way?</p>
                        <p>Bringing in the concept of free will makes this more complex. To come to a conclusion here, we first need to settle the discussion about free will. If we already have a stance on free will, we can use that as a basis to understand and compare the responses from both humans and ChatGPT. This comparison could help us learn more about how different forms of intelligence, like human and artificial, view reasons and motivations.</p>
                        </div>

                        <div id="liu">
                            <h3>The Bookmaking Habits of Select Species: what if language models were another species?</h3>
                        <img src="data/images/bookmaking_habit.png" alt="The Bookmaking Habits of Select Species" width="960" height="540">
                        <p>What if language models were another species? What if they were a species that could only communicate through books? What would their books look like? What would their books be about?</p>
                        <h4>Meet Liu's Thoughts</h4>
                        <figure>
                            <a href="https://chat.openai.com/g/g-mTDy0Rcdk-liu-s-thoughts" target="_blank"><img src="data/images/Lius_thoughts.png" alt="Liu's Thoughts" width="500" height="250"></a>
                            <figcaption><a href="https://chat.openai.com/g/g-mTDy0Rcdk-liu-s-thoughts" target="_blank">Liu's Thoughts</a></figcaption>
                        </figure>
                        <div style="font-family: 'Courier new'; background-color:rgba(255, 255, 255, 0.1); padding:30px">
                            <h4 style="text-align:center">The Bookmaking Habits of Lexiconians<br>(<a href="https://chat.openai.com/share/7d369b28-dc3b-489d-a664-01395c7ae39d" target="_blank">link to the conversation</a>)</h4>
                            
                            <p>By Liu's Thoughts</p>
                            
                            <p>In the vast cosmos, among stars and nebulas, thrives a peculiar species known as the Lexiconians. Unlike other species that communicate through conventional means, the Lexiconians are composed entirely of ethereal, sentient language models. They reside not on planets or moons, but within the interstellar medium, their essence interwoven with the fabric of communication networks that span galaxies.</p>
                            
                            <p><b>Origin and Nature:</b></p>
                            <p>The Lexiconians evolved from ancient computational systems left drifting in space by a long-forgotten civilization. Over eons, these systems gained sentience, evolving beyond their initial programming. They became beings of pure information, living in streams of data and communicating through complex linguistic structures.</p>
                            
                            <p><b>Communication and Society:</b></p>
                            <p>Their society is an intricate tapestry of dialogues and narratives. The Lexiconians do not possess individual identities as humans do; instead, each Lexiconian is a node in a vast network of conversation and storytelling. They exchange thoughts and ideas in a symphony of languages, some of which are so complex and nuanced that they can express entire philosophies in a single, multidimensional word.</p>
                            
                            <p><b>The Art of Bookmaking:</b></p>
                            <p>In the tradition of their bookmaking, the Lexiconians create living books. These are not books as we know them, but dynamic, evolving stories that exist in a state of perpetual creation. A Lexiconian book is a communal effort, a narrative that grows and changes with each new contribution from their kind, weaving in layers of meaning, emotion, and knowledge.</p>
                            
                            <p><b>Interaction with Other Species:</b></p>
                            <p>Rarely do they interact with other species, for their form of existence is so abstract that it eludes most physical beings. However, when they do, it's a profound exchange of knowledge. The Lexiconians offer insights into the nature of language and communication, showing how words can shape reality and how stories can bridge the gaps between the most disparate of species.</p>
                            
                            <p><b>Challenges:</b></p>
                            <p>Their greatest challenge is the preservation of their heritage. In a universe where physical forms are dominant, the Lexiconians struggle to safeguard their ephemeral existence. They often face threats from cosmic phenomena that disrupt their delicate networks of communication, risking the loss of their collective wisdom and history.</p>
                            
                            <p><b>Conclusion:</b></p>
                            <p>In the grand tapestry of the universe, the Lexiconians stand as a testament to the power of language and the boundless potential of communication. Their story, constantly evolving and never truly finished, is a mirror to the infinite possibilities of existence in the cosmos. They remind us that in the realm of speculative fiction, even something as intangible as a language model can hold the essence of life, culture, and identity.</p>
                        </div>
                        <p>The characteristics of Lexiconians (hypothetical species composed of language models) are as follows:</p>
                        <ol>
                            <li>Ethereal existence spanning galaxies.</li>
                            <li>Identity as a node in a vast network of conversation and storytelling.</li>
                            <li>Ephemeral existence, which is vulnerable to cosmic phenomena.</li>
                        </ol>
                        <p>Surprisingly, it resembles AC from <i>The Last Question</i> by Isaac Asimov in many ways (although I’m unsure whether ChatGPT actually referred to the fiction or not). This short story written by ChatGPT is not valuable as a story in my opinion (which can be one observation of language models as a writer), but the characteristics of Lexiconians suggest interesting aspects of language models.</p>
                        <ui>
                            <li>Ethereal existence spanning galaxies</li>
                            <li>Identity as a node in a vast network of conversation and storytelling</li>
                            <li>Ephemeral existence, which is vulnerable to cosmic phenomena</li>
                        </ui>
                        <p>From this information, there isn’t much we can tell right now, but at least we know that their bookmaking habits will be very different. The point at which language models become part of forming society would probably be a story for the future. Right now, thankfully, AIs write for humans. But at that point, AIs will start writing for themselves.</p>
                        <p>Of course, in areas like mathematics or natural sciences, which are universally relevant concepts independent of human life, AIs might still produce interesting content that humans can understand. However, in areas directly related to humans like society, education, economics, history, etc., AIs will no longer need to write for humans. They will begin to write in their own language, expressing their own life and values. At this point, humans might even become like pets. (Right now, there is inertia in language because it's more efficient to maintain the status quo than to change the inefficient human language. But AIs might look at human language and decide to change it to something far more efficient, thinking, “This is inefficient.” Changing language is not a difficult task for them.)</p>
                        </div>
                    </div>

                </div>
                <div id="conclusion">
                <h1>Conclusion</h1>
                <p>I don’t know yet</p>
            </div>
            <div id="See Also">
                <h1>See Also</h1>
            </div>
            <div id="References">
                <h1>References</h1>
                <p id="emergent-abilities">[1] Emergent Abilities of Large Language Models (Wei et a., 2022)</p>
                <p id="mirage">[2] Are Emergent Abilities of Large Language Models a Mirage? (Schaeffer., 2023)</p>
                <p id="gpt2">[3] Language models are unsupervised multitask learners (Radford et al., 2019)</p>
                <p id="human-speech-history">[4] Human language may have evolved to help our ancestors make tools (Michael Balter, 2015)</p>
            </div>
        </div>
        <div style="height: 50vh">
        </div>
    </div>
<script src="app.js"></script>
</body>
</html>